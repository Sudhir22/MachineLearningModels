{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sudhir/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#pip install tensorflow-gpu\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import gensim\n",
    "import tensorflow as tf\n",
    "from keras import Sequential\n",
    "from keras import layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import utils\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend\n",
    "\n",
    "#backend.tensorflow_backend._get_available_gpus()\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Loading the data into a dataframe\n",
    "    \n",
    "    Input\n",
    "    path: path to the test data(String)\n",
    "    \n",
    "    Output\n",
    "    train_data: return a pandas Dataframe\n",
    "    \"\"\"\n",
    "    train_data=pd.read_csv(path)\n",
    "    print(train_data.head())\n",
    "    return train_data\n",
    "\n",
    "#referenced from https://stackoverflow.com/questions/16645799/how-to-create-a-word-cloud-from-a-corpus-in-python\n",
    "def show_wordcloud(data, title = None):\n",
    "    \"\"\"\n",
    "    depicting wordclouds of the input data\n",
    "    \n",
    "    Input\n",
    "    data: input pandas Dataframe\n",
    "    \"\"\"\n",
    "    stopwords = set(STOPWORDS)\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='white',\n",
    "        stopwords=stopwords,\n",
    "        max_words=200,\n",
    "        max_font_size=40, \n",
    "        scale=3,\n",
    "        random_state=1 # chosen at random by flipping a coin; it was heads\n",
    "    ).generate(str(data))\n",
    "\n",
    "    fig = plt.figure(1, figsize=(12, 12))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "\n",
    "def preprocessor(text):\n",
    "    \"\"\"\n",
    "    Tokenizing the sentences using regular expressions and NLTK library\n",
    "    \n",
    "    Input\n",
    "    text: list of descriptions\n",
    "    \n",
    "    Output:\n",
    "    alphabet_tokens: list of tokens\n",
    "    \"\"\"\n",
    "    __tokenization_pattern = r'''(?x)          # set flag to allow verbose regexps\n",
    "        \\$?\\d+(?:\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
    "      | (?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "      | \\w+(?:-\\w+)*        # words with optional internal hyphens\n",
    "      | \\.\\.\\.              # ellipsis\n",
    "      | [][.,;\"'?():_`-]    # these are separate tokens; includes ], [\n",
    "    '''\n",
    "\n",
    "    ## call it using tokenizer.tokenize\n",
    "    tokenizer = nltk.tokenize.regexp.RegexpTokenizer(__tokenization_pattern)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens=[token.lower() for token in tokens if token.isalpha()]\n",
    "    alphabet_tokens = [token for token in tokens if token.isalpha()]\n",
    "    #en_stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    #non_stopwords = [word for word in alphabet_tokens if not word in en_stopwords]\n",
    "    #stemmer = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "    #stems = [str(stemmer.stem(word)) for word in non_stopwords]\n",
    "\n",
    "    return list(alphabet_tokens)\n",
    "\n",
    "\n",
    "\n",
    "def transform_data(train_data):\n",
    "    \"\"\"\n",
    "    Factorizing the simplified lithologies into numerical equivalents\n",
    "    \n",
    "    Input\n",
    "    data: input pandas dataframe\n",
    "    \n",
    "    Output\n",
    "    tuple containing the transformed data\n",
    "    \"\"\"\n",
    "    train_data['Item_Description']=train_data['Item_Description'].replace(np.nan,'',regex=True)\n",
    "    train_data['Item_Description'] = train_data['Item_Description'].apply(preprocessor)\n",
    "    train_data['Product_Category'],uniques=pd.factorize(train_data['Product_Category'])\n",
    "    #train_data['Product_Category']=train_data['Product_Category'].cat.codes\n",
    "    list_of_descriptions=train_data['Item_Description'].tolist()\n",
    "    return (list_of_descriptions,uniques)\n",
    "\n",
    "\n",
    "def generate_embeddings(list_of_descriptions):\n",
    "    \"\"\"\n",
    "    Generating word2vec(vectorized version of each word) model from the vocabulary in the data\n",
    "    \n",
    "    Input\n",
    "    list_of_descriptions: transformed descriptions\n",
    "    list_of_simple_lithology: transformed simple lithologies\n",
    "    \n",
    "    Output\n",
    "    model: Gensim word2vec model\n",
    "\n",
    "    \"\"\"\n",
    "    data=[]\n",
    "    for x in list_of_descriptions:\n",
    "        temp=[]\n",
    "        if(isinstance(x,list)):\n",
    "            for y in x:\n",
    "                temp.append(y.lower())\n",
    "            data.append(temp)\n",
    "    model=gensim.models.FastText(data,min_count=1,size=100,window=3)\n",
    "    return model\n",
    "\n",
    "def split_data(train_data):\n",
    "    \"\"\"\n",
    "    Splitting the data into train and test\n",
    "    \n",
    "    Input\n",
    "    train_data: Pandas dataframe\n",
    "    \n",
    "    Output\n",
    "    tuple containing train and test data \n",
    "    \"\"\"\n",
    "    msk = np.random.rand(len(train_data)) < 0.75\n",
    "    train_X = train_data.Item_Description[msk]\n",
    "    test_X = train_data.Item_Description[~msk]\n",
    "    y=train_data['Product_Category']\n",
    "    train_y = y[msk]\n",
    "    test_y = y[~msk]\n",
    "\n",
    "    return (train_X,train_y,test_X,test_y)\n",
    "\n",
    "\n",
    "def tokenize_input_data(train_X,test_X):\n",
    "    \"\"\"\n",
    "    Indexing each token in the descriptions\n",
    "    \n",
    "    Input\n",
    "    train_X: list of input descriptions\n",
    "    test_X : list of input descriptions\n",
    "    \n",
    "    Output\n",
    "    Tuple containing indexed versions of the inputs\n",
    "    \"\"\"\n",
    "    tokenizer_x=Tokenizer(num_words=3000)    \n",
    "    tokenizer_x.fit_on_texts(train_X)\n",
    "    train_X_transformed=tokenizer_x.texts_to_sequences(train_X)\n",
    "    test_X_transformed=tokenizer_x.texts_to_sequences(test_X)\n",
    "    return (train_X_transformed,test_X_transformed,tokenizer_x)\n",
    "\n",
    "def label_to_id(train_y,test_y):\n",
    "    \"\"\"\n",
    "    Indexing each label in the target(simplified lithology)\n",
    "    \n",
    "    Input\n",
    "    train_y: list of labels\n",
    "    test_y: list of labels\n",
    "    \n",
    "    Output\n",
    "    tuple containing indexed versions of the input\n",
    "    \"\"\"\n",
    "    train_y_transformed=utils.to_categorical(train_y.tolist(),38,dtype='int')\n",
    "    test_y_transformed=utils.to_categorical(test_y.tolist(),38,dtype='int')\n",
    "    return (train_y_transformed,test_y_transformed)\n",
    "\n",
    "\n",
    "def pad_sentences(train_X,test_X,maxlen):\n",
    "    \"\"\"\n",
    "    Adding padding to the descriptions so that each description is of the same length(maxlen)\n",
    "    \n",
    "    Input\n",
    "    train_X: list of descriptions\n",
    "    test_X: list of descriptions\n",
    "    maxlen: int (maximum length of the descriptions)\n",
    "    \n",
    "    Output\n",
    "    Tuple containing transformed versions of the input\n",
    "    \"\"\"\n",
    "    train_X_transformed= pad_sequences(train_X, padding='post', maxlen=maxlen)\n",
    "    test_X_transformed= pad_sequences(test_X, padding='post', maxlen=maxlen)\n",
    "    return (train_X_transformed,test_X_transformed)\n",
    "    \n",
    "\n",
    "def create_embedding_matrix(model,tokenizer):\n",
    "    \"\"\"\n",
    "    Creating an embedding matrix to be fed into the neural network\n",
    "    \n",
    "    Input\n",
    "    model: gensim word2vec model\n",
    "    \n",
    "    embedding_matrix: matrix depicting the embeddings\n",
    "    \"\"\"\n",
    "    embedding_matrix=np.zeros((len(model.wv.vocab),100))\n",
    "    for x,y in model.wv.vocab.items():\n",
    "        if x in tokenizer.word_counts.keys():\n",
    "            embedding_matrix[tokenizer.word_index[x]]=np.array(model.wv[x], dtype=np.float32)[:100]\n",
    "    \n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def define_learning_model(model,embedding_matrix,maxlen):\n",
    "    \"\"\"\n",
    "    Describing the deep learning model using Keras\n",
    "    \n",
    "    Input\n",
    "    model:gensim word2vec model\n",
    "    embedding_matrix: matrix of embeddings\n",
    "    maxlen: maximum length of sentences\n",
    "    \n",
    "    Output\n",
    "    lstm_model: deep learning model\n",
    "    \"\"\"\n",
    "    lstm_model=Sequential()\n",
    "    lstm_model.add(layers.Embedding(len(model.wv.vocab), 100, \n",
    "                               weights=[embedding_matrix],\n",
    "                               input_length=maxlen,\n",
    "                               trainable=False))\n",
    "    lstm_model.add(layers.LSTM(100))\n",
    "    #model.add(layers.Dropout(0.3))\n",
    "    #model.add(layers.LSTM(100,activation='tanh',recurrent_activation='sigmoid'))\n",
    "    lstm_model.add(layers.Dropout(0.3))\n",
    "\n",
    "    #model.add(layers.GlobalAveragePooling1D())\n",
    "    lstm_model.add(layers.Dense(38,activation='softmax'))\n",
    "    #model.add(layers.Flatten())\n",
    "    adam=optimizers.Adam(lr=0.001)\n",
    "    lstm_model.compile(optimizer=adam,\n",
    "                  loss='mse',\n",
    "                  metrics=['accuracy'])\n",
    "    lstm_model.summary()\n",
    "    return lstm_model\n",
    "\n",
    "def calculate_accuracy(train_X,train_y,test_X,test_y,model):\n",
    "    \"\"\"\n",
    "    Calculating the accuracy of the model.\n",
    "    \n",
    "    Input\n",
    "    train_X: list of descriptions\n",
    "    train_y: list of labels\n",
    "    \n",
    "    Output:\n",
    "    history: model after fitting the data\n",
    "    \n",
    "    \"\"\"\n",
    "    msk=np.random.randn(len(train_X))<0.75\n",
    "    validation_data_X=train_X[~msk]\n",
    "    validation_data_Y=train_y[~msk]\n",
    "    history = model.fit(train_X[msk],train_y[msk],\n",
    "                        epochs=10,\n",
    "                        verbose=2,\n",
    "                       validation_data=(validation_data_X,validation_data_Y))\n",
    "    loss, accuracy = model.evaluate(train_X, train_y, verbose=False)\n",
    "    print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "    loss, accuracy = model.evaluate(test_X, test_y, verbose=False)\n",
    "    print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "    \n",
    "    return (history,accuracy)\n",
    "\n",
    "#used as reference from https://www.tensorflow.org/tutorials/keras/basic_text_classification\n",
    "def plot_loss(model):\n",
    "    \"\"\"\n",
    "    Plot the training and validation loss w.r.t epochs\n",
    "    \n",
    "    Input\n",
    "    model: deep learning model\n",
    "    \"\"\"\n",
    "    history_dict = history.history\n",
    "    history_dict.keys()\n",
    "    loss = history_dict['loss']\n",
    "    val_loss = history_dict['val_loss']\n",
    "\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "\n",
    "    # \"bo\" is for \"blue dot\"\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    # b is for \"solid blue line\"\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv('Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Inv_Id Vendor_Code     GL_Code    Inv_Amt  \\\n",
      "0       1   VENDOR-61  GL-6050100   6.973473   \n",
      "1       2   VENDOR-61  GL-6050100  25.053841   \n",
      "2       3  VENDOR-449  GL-6050100  53.573737   \n",
      "3       4  VENDOR-682  GL-6050100  67.388827   \n",
      "4       5  VENDOR-682  GL-6050100  74.262047   \n",
      "\n",
      "                                    Item_Description Product_Category  \n",
      "0  AETNA VARIABLE FUND - Apr-2002 - Store Managem...        CLASS-784  \n",
      "1  AETNA VARIABLE FUND - Nov-2000 - Store Managem...        CLASS-784  \n",
      "2  FAIRCHILD CORP - Nov-2001 - Store Management R...        CLASS-784  \n",
      "3  CALIFORNIA REAL ESTATE INVESTMENT TRUST - Aug-...        CLASS-784  \n",
      "4  CALIFORNIA REAL ESTATE INVESTMENT TRUST - Mar-...        CLASS-784  \n"
     ]
    }
   ],
   "source": [
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">Inv_Amt</th>\n",
       "      <th colspan=\"8\" halign=\"left\">Inv_Id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Product_Category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CLASS-1042</th>\n",
       "      <td>34.0</td>\n",
       "      <td>60.974592</td>\n",
       "      <td>25.748951</td>\n",
       "      <td>1.248599</td>\n",
       "      <td>41.644535</td>\n",
       "      <td>66.096761</td>\n",
       "      <td>80.621092</td>\n",
       "      <td>98.328181</td>\n",
       "      <td>34.0</td>\n",
       "      <td>697.617647</td>\n",
       "      <td>163.916648</td>\n",
       "      <td>556.0</td>\n",
       "      <td>568.25</td>\n",
       "      <td>581.5</td>\n",
       "      <td>899.75</td>\n",
       "      <td>909.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS-110</th>\n",
       "      <td>29.0</td>\n",
       "      <td>57.214454</td>\n",
       "      <td>29.463161</td>\n",
       "      <td>1.654231</td>\n",
       "      <td>28.918789</td>\n",
       "      <td>70.081291</td>\n",
       "      <td>78.289335</td>\n",
       "      <td>98.174961</td>\n",
       "      <td>29.0</td>\n",
       "      <td>935.206897</td>\n",
       "      <td>13.243271</td>\n",
       "      <td>914.0</td>\n",
       "      <td>925.00</td>\n",
       "      <td>935.0</td>\n",
       "      <td>948.00</td>\n",
       "      <td>956.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS-123</th>\n",
       "      <td>26.0</td>\n",
       "      <td>46.571002</td>\n",
       "      <td>26.750018</td>\n",
       "      <td>0.700048</td>\n",
       "      <td>31.596581</td>\n",
       "      <td>48.117384</td>\n",
       "      <td>66.709427</td>\n",
       "      <td>96.044006</td>\n",
       "      <td>26.0</td>\n",
       "      <td>5785.461538</td>\n",
       "      <td>2087.901650</td>\n",
       "      <td>115.0</td>\n",
       "      <td>6512.00</td>\n",
       "      <td>6521.5</td>\n",
       "      <td>6533.75</td>\n",
       "      <td>6544.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS-177</th>\n",
       "      <td>370.0</td>\n",
       "      <td>48.894782</td>\n",
       "      <td>27.985017</td>\n",
       "      <td>0.080097</td>\n",
       "      <td>24.439381</td>\n",
       "      <td>47.754154</td>\n",
       "      <td>73.085842</td>\n",
       "      <td>99.890310</td>\n",
       "      <td>370.0</td>\n",
       "      <td>395.454054</td>\n",
       "      <td>173.694369</td>\n",
       "      <td>114.0</td>\n",
       "      <td>254.25</td>\n",
       "      <td>379.5</td>\n",
       "      <td>510.75</td>\n",
       "      <td>736.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS-230</th>\n",
       "      <td>34.0</td>\n",
       "      <td>55.326168</td>\n",
       "      <td>28.631577</td>\n",
       "      <td>1.992731</td>\n",
       "      <td>32.449521</td>\n",
       "      <td>53.825759</td>\n",
       "      <td>76.759353</td>\n",
       "      <td>98.441856</td>\n",
       "      <td>34.0</td>\n",
       "      <td>7952.441176</td>\n",
       "      <td>16.951251</td>\n",
       "      <td>7925.0</td>\n",
       "      <td>7940.25</td>\n",
       "      <td>7950.0</td>\n",
       "      <td>7966.75</td>\n",
       "      <td>7984.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS-278</th>\n",
       "      <td>50.0</td>\n",
       "      <td>45.298282</td>\n",
       "      <td>24.010621</td>\n",
       "      <td>0.513286</td>\n",
       "      <td>30.048048</td>\n",
       "      <td>45.052260</td>\n",
       "      <td>62.150757</td>\n",
       "      <td>93.941683</td>\n",
       "      <td>50.0</td>\n",
       "      <td>993.320000</td>\n",
       "      <td>22.003191</td>\n",
       "      <td>957.0</td>\n",
       "      <td>974.25</td>\n",
       "      <td>991.5</td>\n",
       "      <td>1010.75</td>\n",
       "      <td>1033.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS-322</th>\n",
       "      <td>107.0</td>\n",
       "      <td>50.695708</td>\n",
       "      <td>27.877681</td>\n",
       "      <td>1.684283</td>\n",
       "      <td>25.408636</td>\n",
       "      <td>50.336449</td>\n",
       "      <td>73.290300</td>\n",
       "      <td>98.886448</td>\n",
       "      <td>107.0</td>\n",
       "      <td>7259.654206</td>\n",
       "      <td>322.693005</td>\n",
       "      <td>6549.0</td>\n",
       "      <td>7068.50</td>\n",
       "      <td>7232.0</td>\n",
       "      <td>7449.00</td>\n",
       "      <td>7790.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS-323</th>\n",
       "      <td>773.0</td>\n",
       "      <td>51.228272</td>\n",
       "      <td>29.204031</td>\n",
       "      <td>0.144950</td>\n",
       "      <td>25.071323</td>\n",
       "      <td>53.654391</td>\n",
       "      <td>75.803609</td>\n",
       "      <td>99.541335</td>\n",
       "      <td>773.0</td>\n",
       "      <td>7166.063389</td>\n",
       "      <td>363.273634</td>\n",
       "      <td>6545.0</td>\n",
       "      <td>6839.00</td>\n",
       "      <td>7157.0</td>\n",
       "      <td>7488.00</td>\n",
       "      <td>7791.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS-368</th>\n",
       "      <td>79.0</td>\n",
       "      <td>51.232969</td>\n",
       "      <td>28.352466</td>\n",
       "      <td>2.888632</td>\n",
       "      <td>28.662821</td>\n",
       "      <td>52.136653</td>\n",
       "      <td>74.168341</td>\n",
       "      <td>99.356221</td>\n",
       "      <td>79.0</td>\n",
       "      <td>813.898734</td>\n",
       "      <td>229.573718</td>\n",
       "      <td>33.0</td>\n",
       "      <td>781.50</td>\n",
       "      <td>812.0</td>\n",
       "      <td>839.50</td>\n",
       "      <td>1499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS-453</th>\n",
       "      <td>15.0</td>\n",
       "      <td>55.659948</td>\n",
       "      <td>33.572944</td>\n",
       "      <td>4.650694</td>\n",
       "      <td>23.204797</td>\n",
       "      <td>62.504676</td>\n",
       "      <td>84.507380</td>\n",
       "      <td>99.437946</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3796.533333</td>\n",
       "      <td>2990.524748</td>\n",
       "      <td>554.0</td>\n",
       "      <td>748.00</td>\n",
       "      <td>6493.0</td>\n",
       "      <td>6497.50</td>\n",
       "      <td>6503.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS-489</th>\n",
       "      <td>4.0</td>\n",
       "      <td>60.187186</td>\n",
       "      <td>40.369151</td>\n",
       "      <td>22.020404</td>\n",
       "      <td>27.215569</td>\n",
       "      <td>59.476387</td>\n",
       "      <td>92.448004</td>\n",
       "      <td>99.775565</td>\n",
       "      <td>4.0</td>\n",
       "      <td>28.250000</td>\n",
       "      <td>1.707825</td>\n",
       "      <td>26.0</td>\n",
       "      <td>27.50</td>\n",
       "      <td>28.5</td>\n",
       "      <td>29.25</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS-49</th>\n",
       "      <td>7.0</td>\n",
       "      <td>47.667863</td>\n",
       "      <td>34.696608</td>\n",
       "      <td>3.362255</td>\n",
       "      <td>23.041988</td>\n",
       "      <td>37.752404</td>\n",
       "      <td>78.461761</td>\n",
       "      <td>89.552884</td>\n",
       "      <td>7.0</td>\n",
       "      <td>740.714286</td>\n",
       "      <td>52.375930</td>\n",
       "      <td>622.0</td>\n",
       "      <td>758.50</td>\n",
       "      <td>760.0</td>\n",
       "      <td>761.50</td>\n",
       "      <td>763.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS-50</th>\n",
       "      <td>196.0</td>\n",
       "      <td>47.238017</td>\n",
       "      <td>28.150080</td>\n",
       "      <td>0.530884</td>\n",
       "      <td>24.160029</td>\n",
       "      <td>43.520568</td>\n",
       "      <td>69.906562</td>\n",
       "      <td>98.578344</td>\n",
       "      <td>196.0</td>\n",
       "      <td>2147.168367</td>\n",
       "      <td>73.320772</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>2083.75</td>\n",
       "      <td>2147.5</td>\n",
       "      <td>2211.25</td>\n",
       "      <td>2270.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS-51</th>\n",
       "      <td>464.0</td>\n",
       "      <td>52.913867</td>\n",
       "      <td>29.777899</td>\n",
       "      <td>0.134731</td>\n",
       "      <td>26.849229</td>\n",
       "      <td>54.998350</td>\n",
       "      <td>79.816269</td>\n",
       "      <td>99.889537</td>\n",
       "      <td>464.0</td>\n",
       "      <td>2597.525862</td>\n",
       "      <td>187.144079</td>\n",
       "      <td>2272.0</td>\n",
       "      <td>2435.50</td>\n",
       "      <td>2600.5</td>\n",
       "      <td>2757.25</td>\n",
       "      <td>2913.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS-522</th>\n",
       "      <td>117.0</td>\n",
       "      <td>46.839683</td>\n",
       "      <td>27.004984</td>\n",
       "      <td>0.165818</td>\n",
       "      <td>25.723111</td>\n",
       "      <td>47.435456</td>\n",
       "      <td>66.992695</td>\n",
       "      <td>99.891528</td>\n",
       "      <td>117.0</td>\n",
       "      <td>1117.735043</td>\n",
       "      <td>46.316265</td>\n",
       "      <td>1038.0</td>\n",
       "      <td>1080.00</td>\n",
       "      <td>1114.0</td>\n",
       "      <td>1159.00</td>\n",
       "      <td>1197.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS-559</th>\n",
       "      <td>1521.0</td>\n",
       "      <td>48.992146</td>\n",
       "      <td>28.300972</td>\n",
       "      <td>0.136878</td>\n",
       "      <td>24.257341</td>\n",
       "      <td>48.580820</td>\n",
       "      <td>72.545172</td>\n",
       "      <td>99.964731</td>\n",
       "      <td>1521.0</td>\n",
       "      <td>3990.015779</td>\n",
       "      <td>613.849273</td>\n",
       "      <td>2916.0</td>\n",
       "      <td>3459.00</td>\n",
       "      <td>3987.0</td>\n",
       "      <td>4524.00</td>\n",
       "      <td>5052.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS-571</th>\n",
       "      <td>22.0</td>\n",
       "      <td>55.731305</td>\n",
       "      <td>24.107022</td>\n",
       "      <td>19.232263</td>\n",
       "      <td>37.144914</td>\n",
       "      <td>52.381067</td>\n",
       "      <td>71.643600</td>\n",
       "      <td>99.841762</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7999.590909</td>\n",
       "      <td>7.938208</td>\n",
       "      <td>7986.0</td>\n",
       "      <td>7994.25</td>\n",
       "      <td>8000.5</td>\n",
       "      <td>8005.75</td>\n",
       "      <td>8012.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS-606</th>\n",
       "      <td>13.0</td>\n",
       "      <td>44.112473</td>\n",
       "      <td>28.870722</td>\n",
       "      <td>1.717730</td>\n",
       "      <td>18.210818</td>\n",
       "      <td>40.317636</td>\n",
       "      <td>66.354096</td>\n",
       "      <td>85.402233</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1208.076923</td>\n",
       "      <td>6.075381</td>\n",
       "      <td>1199.0</td>\n",
       "      <td>1202.00</td>\n",
       "      <td>1210.0</td>\n",
       "      <td>1213.00</td>\n",
       "      <td>1216.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS-629</th>\n",
       "      <td>115.0</td>\n",
       "      <td>48.248185</td>\n",
       "      <td>29.250721</td>\n",
       "      <td>0.024427</td>\n",
       "      <td>23.029758</td>\n",
       "      <td>46.077755</td>\n",
       "      <td>73.320072</td>\n",
       "      <td>99.023501</td>\n",
       "      <td>115.0</td>\n",
       "      <td>737.513043</td>\n",
       "      <td>623.603540</td>\n",
       "      <td>41.0</td>\n",
       "      <td>83.50</td>\n",
       "      <td>1280.0</td>\n",
       "      <td>1325.50</td>\n",
       "      <td>1363.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS-651</th>\n",
       "      <td>68.0</td>\n",
       "      <td>55.835358</td>\n",
       "      <td>26.357492</td>\n",
       "      <td>7.617872</td>\n",
       "      <td>33.320754</td>\n",
       "      <td>61.619228</td>\n",
       "      <td>76.804049</td>\n",
       "      <td>96.711920</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1374.029412</td>\n",
       "      <td>78.429923</td>\n",
       "      <td>1218.0</td>\n",
       "      <td>1378.75</td>\n",
       "      <td>1405.0</td>\n",
       "      <td>1426.25</td>\n",
       "      <td>1446.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS-668</th>\n",
       "      <td>13.0</td>\n",
       "      <td>50.120592</td>\n",
       "      <td>29.120956</td>\n",
       "      <td>12.973565</td>\n",
       "      <td>21.773719</td>\n",
       "      <td>40.620103</td>\n",
       "      <td>74.816404</td>\n",
       "      <td>97.566321</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1476.076923</td>\n",
       "      <td>7.017378</td>\n",
       "      <td>1463.0</td>\n",
       "      <td>1472.00</td>\n",
       "      <td>1477.0</td>\n",
       "      <td>1482.00</td>\n",
       "      <td>1485.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS-671</th>\n",
       "      <td>42.0</td>\n",
       "      <td>52.277491</td>\n",
       "      <td>30.656421</td>\n",
       "      <td>0.394850</td>\n",
       "      <td>25.834286</td>\n",
       "      <td>54.053423</td>\n",
       "      <td>80.954545</td>\n",
       "      <td>99.723069</td>\n",
       "      <td>42.0</td>\n",
       "      <td>4575.190476</td>\n",
       "      <td>3538.938054</td>\n",
       "      <td>483.0</td>\n",
       "      <td>841.00</td>\n",
       "      <td>7864.5</td>\n",
       "      <td>7896.75</td>\n",
       "      <td>7978.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS-720</th>\n",
       "      <td>73.0</td>\n",
       "      <td>44.731442</td>\n",
       "      <td>30.237553</td>\n",
       "      <td>0.301399</td>\n",
       "      <td>18.376718</td>\n",
       "      <td>43.715027</td>\n",
       "      <td>69.820081</td>\n",
       "      <td>99.560459</td>\n",
       "      <td>73.0</td>\n",
       "      <td>7850.219178</td>\n",
       "      <td>38.322552</td>\n",
       "      <td>7792.0</td>\n",
       "      <td>7818.00</td>\n",
       "      <td>7847.0</td>\n",
       "      <td>7874.00</td>\n",
       "      <td>7922.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS-74</th>\n",
       "      <td>42.0</td>\n",
       "      <td>48.452051</td>\n",
       "      <td>28.920911</td>\n",
       "      <td>2.190137</td>\n",
       "      <td>25.516144</td>\n",
       "      <td>44.849803</td>\n",
       "      <td>69.515789</td>\n",
       "      <td>99.895282</td>\n",
       "      <td>42.0</td>\n",
       "      <td>6228.190476</td>\n",
       "      <td>307.678264</td>\n",
       "      <td>5071.0</td>\n",
       "      <td>6249.50</td>\n",
       "      <td>6326.5</td>\n",
       "      <td>6341.50</td>\n",
       "      <td>6490.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS-75</th>\n",
       "      <td>985.0</td>\n",
       "      <td>51.062387</td>\n",
       "      <td>27.963813</td>\n",
       "      <td>0.187873</td>\n",
       "      <td>27.215340</td>\n",
       "      <td>53.214177</td>\n",
       "      <td>74.277252</td>\n",
       "      <td>99.975850</td>\n",
       "      <td>985.0</td>\n",
       "      <td>5748.098477</td>\n",
       "      <td>413.303971</td>\n",
       "      <td>5054.0</td>\n",
       "      <td>5392.00</td>\n",
       "      <td>5734.0</td>\n",
       "      <td>6096.00</td>\n",
       "      <td>6486.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS-758</th>\n",
       "      <td>3.0</td>\n",
       "      <td>46.674575</td>\n",
       "      <td>35.172405</td>\n",
       "      <td>17.605687</td>\n",
       "      <td>27.125783</td>\n",
       "      <td>36.645878</td>\n",
       "      <td>61.209019</td>\n",
       "      <td>85.772159</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5772.666667</td>\n",
       "      <td>3691.611346</td>\n",
       "      <td>1511.0</td>\n",
       "      <td>4666.50</td>\n",
       "      <td>7822.0</td>\n",
       "      <td>7903.50</td>\n",
       "      <td>7985.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS-764</th>\n",
       "      <td>219.0</td>\n",
       "      <td>50.597506</td>\n",
       "      <td>27.032173</td>\n",
       "      <td>0.550979</td>\n",
       "      <td>28.549009</td>\n",
       "      <td>50.521839</td>\n",
       "      <td>75.036500</td>\n",
       "      <td>98.021158</td>\n",
       "      <td>219.0</td>\n",
       "      <td>1839.872146</td>\n",
       "      <td>100.084240</td>\n",
       "      <td>1666.0</td>\n",
       "      <td>1758.50</td>\n",
       "      <td>1838.0</td>\n",
       "      <td>1929.50</td>\n",
       "      <td>2012.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS-765</th>\n",
       "      <td>27.0</td>\n",
       "      <td>36.734164</td>\n",
       "      <td>29.229054</td>\n",
       "      <td>0.152828</td>\n",
       "      <td>14.202381</td>\n",
       "      <td>24.885379</td>\n",
       "      <td>65.059179</td>\n",
       "      <td>95.567080</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1253.333333</td>\n",
       "      <td>11.783300</td>\n",
       "      <td>1235.0</td>\n",
       "      <td>1242.50</td>\n",
       "      <td>1254.0</td>\n",
       "      <td>1262.50</td>\n",
       "      <td>1272.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS-783</th>\n",
       "      <td>2.0</td>\n",
       "      <td>58.712751</td>\n",
       "      <td>39.007570</td>\n",
       "      <td>31.130234</td>\n",
       "      <td>44.921493</td>\n",
       "      <td>58.712751</td>\n",
       "      <td>72.504010</td>\n",
       "      <td>86.295269</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6491.500000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>6491.0</td>\n",
       "      <td>6491.25</td>\n",
       "      <td>6491.5</td>\n",
       "      <td>6491.75</td>\n",
       "      <td>6492.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS-784</th>\n",
       "      <td>19.0</td>\n",
       "      <td>47.277801</td>\n",
       "      <td>29.060783</td>\n",
       "      <td>5.520218</td>\n",
       "      <td>21.953292</td>\n",
       "      <td>53.573737</td>\n",
       "      <td>70.825437</td>\n",
       "      <td>97.217357</td>\n",
       "      <td>19.0</td>\n",
       "      <td>22.526316</td>\n",
       "      <td>34.860625</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>11.0</td>\n",
       "      <td>19.00</td>\n",
       "      <td>120.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS-804</th>\n",
       "      <td>38.0</td>\n",
       "      <td>47.307416</td>\n",
       "      <td>31.017443</td>\n",
       "      <td>0.453047</td>\n",
       "      <td>22.721458</td>\n",
       "      <td>47.134540</td>\n",
       "      <td>75.426874</td>\n",
       "      <td>99.074300</td>\n",
       "      <td>38.0</td>\n",
       "      <td>804.368421</td>\n",
       "      <td>94.515098</td>\n",
       "      <td>521.0</td>\n",
       "      <td>744.25</td>\n",
       "      <td>868.0</td>\n",
       "      <td>879.75</td>\n",
       "      <td>893.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS-816</th>\n",
       "      <td>2.0</td>\n",
       "      <td>39.904279</td>\n",
       "      <td>9.439308</td>\n",
       "      <td>33.229680</td>\n",
       "      <td>36.566980</td>\n",
       "      <td>39.904279</td>\n",
       "      <td>43.241578</td>\n",
       "      <td>46.578878</td>\n",
       "      <td>2.0</td>\n",
       "      <td>38.500000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>38.0</td>\n",
       "      <td>38.25</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38.75</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS-839</th>\n",
       "      <td>13.0</td>\n",
       "      <td>51.082406</td>\n",
       "      <td>32.746551</td>\n",
       "      <td>0.612597</td>\n",
       "      <td>37.203025</td>\n",
       "      <td>44.640020</td>\n",
       "      <td>80.370893</td>\n",
       "      <td>95.416308</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1454.230769</td>\n",
       "      <td>5.182564</td>\n",
       "      <td>1447.0</td>\n",
       "      <td>1450.00</td>\n",
       "      <td>1454.0</td>\n",
       "      <td>1459.00</td>\n",
       "      <td>1462.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS-913</th>\n",
       "      <td>53.0</td>\n",
       "      <td>54.476749</td>\n",
       "      <td>30.538769</td>\n",
       "      <td>0.357484</td>\n",
       "      <td>30.770910</td>\n",
       "      <td>57.333652</td>\n",
       "      <td>81.188005</td>\n",
       "      <td>99.450833</td>\n",
       "      <td>53.0</td>\n",
       "      <td>2460.320755</td>\n",
       "      <td>2269.096201</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1504.00</td>\n",
       "      <td>1781.0</td>\n",
       "      <td>1927.00</td>\n",
       "      <td>7981.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS-942</th>\n",
       "      <td>107.0</td>\n",
       "      <td>51.167536</td>\n",
       "      <td>29.123929</td>\n",
       "      <td>0.238615</td>\n",
       "      <td>25.693293</td>\n",
       "      <td>57.623250</td>\n",
       "      <td>76.850597</td>\n",
       "      <td>98.655611</td>\n",
       "      <td>107.0</td>\n",
       "      <td>1588.056075</td>\n",
       "      <td>44.876315</td>\n",
       "      <td>1513.0</td>\n",
       "      <td>1552.00</td>\n",
       "      <td>1587.0</td>\n",
       "      <td>1627.50</td>\n",
       "      <td>1665.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS-947</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.577323</td>\n",
       "      <td>0.349651</td>\n",
       "      <td>1.330082</td>\n",
       "      <td>1.453703</td>\n",
       "      <td>1.577323</td>\n",
       "      <td>1.700943</td>\n",
       "      <td>1.824563</td>\n",
       "      <td>2.0</td>\n",
       "      <td>665.500000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>665.0</td>\n",
       "      <td>665.25</td>\n",
       "      <td>665.5</td>\n",
       "      <td>665.75</td>\n",
       "      <td>666.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS-95</th>\n",
       "      <td>27.0</td>\n",
       "      <td>38.834801</td>\n",
       "      <td>27.026628</td>\n",
       "      <td>0.354302</td>\n",
       "      <td>14.640159</td>\n",
       "      <td>46.754812</td>\n",
       "      <td>55.385996</td>\n",
       "      <td>94.246199</td>\n",
       "      <td>27.0</td>\n",
       "      <td>604.666667</td>\n",
       "      <td>11.104400</td>\n",
       "      <td>587.0</td>\n",
       "      <td>594.50</td>\n",
       "      <td>605.0</td>\n",
       "      <td>614.50</td>\n",
       "      <td>621.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASS-953</th>\n",
       "      <td>8.0</td>\n",
       "      <td>50.237633</td>\n",
       "      <td>23.392879</td>\n",
       "      <td>24.739122</td>\n",
       "      <td>29.576064</td>\n",
       "      <td>45.512420</td>\n",
       "      <td>69.533807</td>\n",
       "      <td>82.696309</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1368.875000</td>\n",
       "      <td>2.900123</td>\n",
       "      <td>1365.0</td>\n",
       "      <td>1366.75</td>\n",
       "      <td>1368.5</td>\n",
       "      <td>1371.25</td>\n",
       "      <td>1373.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Inv_Amt                                              \\\n",
       "                   count       mean        std        min        25%   \n",
       "Product_Category                                                       \n",
       "CLASS-1042          34.0  60.974592  25.748951   1.248599  41.644535   \n",
       "CLASS-110           29.0  57.214454  29.463161   1.654231  28.918789   \n",
       "CLASS-123           26.0  46.571002  26.750018   0.700048  31.596581   \n",
       "CLASS-177          370.0  48.894782  27.985017   0.080097  24.439381   \n",
       "CLASS-230           34.0  55.326168  28.631577   1.992731  32.449521   \n",
       "CLASS-278           50.0  45.298282  24.010621   0.513286  30.048048   \n",
       "CLASS-322          107.0  50.695708  27.877681   1.684283  25.408636   \n",
       "CLASS-323          773.0  51.228272  29.204031   0.144950  25.071323   \n",
       "CLASS-368           79.0  51.232969  28.352466   2.888632  28.662821   \n",
       "CLASS-453           15.0  55.659948  33.572944   4.650694  23.204797   \n",
       "CLASS-489            4.0  60.187186  40.369151  22.020404  27.215569   \n",
       "CLASS-49             7.0  47.667863  34.696608   3.362255  23.041988   \n",
       "CLASS-50           196.0  47.238017  28.150080   0.530884  24.160029   \n",
       "CLASS-51           464.0  52.913867  29.777899   0.134731  26.849229   \n",
       "CLASS-522          117.0  46.839683  27.004984   0.165818  25.723111   \n",
       "CLASS-559         1521.0  48.992146  28.300972   0.136878  24.257341   \n",
       "CLASS-571           22.0  55.731305  24.107022  19.232263  37.144914   \n",
       "CLASS-606           13.0  44.112473  28.870722   1.717730  18.210818   \n",
       "CLASS-629          115.0  48.248185  29.250721   0.024427  23.029758   \n",
       "CLASS-651           68.0  55.835358  26.357492   7.617872  33.320754   \n",
       "CLASS-668           13.0  50.120592  29.120956  12.973565  21.773719   \n",
       "CLASS-671           42.0  52.277491  30.656421   0.394850  25.834286   \n",
       "CLASS-720           73.0  44.731442  30.237553   0.301399  18.376718   \n",
       "CLASS-74            42.0  48.452051  28.920911   2.190137  25.516144   \n",
       "CLASS-75           985.0  51.062387  27.963813   0.187873  27.215340   \n",
       "CLASS-758            3.0  46.674575  35.172405  17.605687  27.125783   \n",
       "CLASS-764          219.0  50.597506  27.032173   0.550979  28.549009   \n",
       "CLASS-765           27.0  36.734164  29.229054   0.152828  14.202381   \n",
       "CLASS-783            2.0  58.712751  39.007570  31.130234  44.921493   \n",
       "CLASS-784           19.0  47.277801  29.060783   5.520218  21.953292   \n",
       "CLASS-804           38.0  47.307416  31.017443   0.453047  22.721458   \n",
       "CLASS-816            2.0  39.904279   9.439308  33.229680  36.566980   \n",
       "CLASS-839           13.0  51.082406  32.746551   0.612597  37.203025   \n",
       "CLASS-913           53.0  54.476749  30.538769   0.357484  30.770910   \n",
       "CLASS-942          107.0  51.167536  29.123929   0.238615  25.693293   \n",
       "CLASS-947            2.0   1.577323   0.349651   1.330082   1.453703   \n",
       "CLASS-95            27.0  38.834801  27.026628   0.354302  14.640159   \n",
       "CLASS-953            8.0  50.237633  23.392879  24.739122  29.576064   \n",
       "\n",
       "                                                   Inv_Id               \\\n",
       "                        50%        75%        max   count         mean   \n",
       "Product_Category                                                         \n",
       "CLASS-1042        66.096761  80.621092  98.328181    34.0   697.617647   \n",
       "CLASS-110         70.081291  78.289335  98.174961    29.0   935.206897   \n",
       "CLASS-123         48.117384  66.709427  96.044006    26.0  5785.461538   \n",
       "CLASS-177         47.754154  73.085842  99.890310   370.0   395.454054   \n",
       "CLASS-230         53.825759  76.759353  98.441856    34.0  7952.441176   \n",
       "CLASS-278         45.052260  62.150757  93.941683    50.0   993.320000   \n",
       "CLASS-322         50.336449  73.290300  98.886448   107.0  7259.654206   \n",
       "CLASS-323         53.654391  75.803609  99.541335   773.0  7166.063389   \n",
       "CLASS-368         52.136653  74.168341  99.356221    79.0   813.898734   \n",
       "CLASS-453         62.504676  84.507380  99.437946    15.0  3796.533333   \n",
       "CLASS-489         59.476387  92.448004  99.775565     4.0    28.250000   \n",
       "CLASS-49          37.752404  78.461761  89.552884     7.0   740.714286   \n",
       "CLASS-50          43.520568  69.906562  98.578344   196.0  2147.168367   \n",
       "CLASS-51          54.998350  79.816269  99.889537   464.0  2597.525862   \n",
       "CLASS-522         47.435456  66.992695  99.891528   117.0  1117.735043   \n",
       "CLASS-559         48.580820  72.545172  99.964731  1521.0  3990.015779   \n",
       "CLASS-571         52.381067  71.643600  99.841762    22.0  7999.590909   \n",
       "CLASS-606         40.317636  66.354096  85.402233    13.0  1208.076923   \n",
       "CLASS-629         46.077755  73.320072  99.023501   115.0   737.513043   \n",
       "CLASS-651         61.619228  76.804049  96.711920    68.0  1374.029412   \n",
       "CLASS-668         40.620103  74.816404  97.566321    13.0  1476.076923   \n",
       "CLASS-671         54.053423  80.954545  99.723069    42.0  4575.190476   \n",
       "CLASS-720         43.715027  69.820081  99.560459    73.0  7850.219178   \n",
       "CLASS-74          44.849803  69.515789  99.895282    42.0  6228.190476   \n",
       "CLASS-75          53.214177  74.277252  99.975850   985.0  5748.098477   \n",
       "CLASS-758         36.645878  61.209019  85.772159     3.0  5772.666667   \n",
       "CLASS-764         50.521839  75.036500  98.021158   219.0  1839.872146   \n",
       "CLASS-765         24.885379  65.059179  95.567080    27.0  1253.333333   \n",
       "CLASS-783         58.712751  72.504010  86.295269     2.0  6491.500000   \n",
       "CLASS-784         53.573737  70.825437  97.217357    19.0    22.526316   \n",
       "CLASS-804         47.134540  75.426874  99.074300    38.0   804.368421   \n",
       "CLASS-816         39.904279  43.241578  46.578878     2.0    38.500000   \n",
       "CLASS-839         44.640020  80.370893  95.416308    13.0  1454.230769   \n",
       "CLASS-913         57.333652  81.188005  99.450833    53.0  2460.320755   \n",
       "CLASS-942         57.623250  76.850597  98.655611   107.0  1588.056075   \n",
       "CLASS-947          1.577323   1.700943   1.824563     2.0   665.500000   \n",
       "CLASS-95          46.754812  55.385996  94.246199    27.0   604.666667   \n",
       "CLASS-953         45.512420  69.533807  82.696309     8.0  1368.875000   \n",
       "\n",
       "                                                                         \n",
       "                          std     min      25%     50%      75%     max  \n",
       "Product_Category                                                         \n",
       "CLASS-1042         163.916648   556.0   568.25   581.5   899.75   909.0  \n",
       "CLASS-110           13.243271   914.0   925.00   935.0   948.00   956.0  \n",
       "CLASS-123         2087.901650   115.0  6512.00  6521.5  6533.75  6544.0  \n",
       "CLASS-177          173.694369   114.0   254.25   379.5   510.75   736.0  \n",
       "CLASS-230           16.951251  7925.0  7940.25  7950.0  7966.75  7984.0  \n",
       "CLASS-278           22.003191   957.0   974.25   991.5  1010.75  1033.0  \n",
       "CLASS-322          322.693005  6549.0  7068.50  7232.0  7449.00  7790.0  \n",
       "CLASS-323          363.273634  6545.0  6839.00  7157.0  7488.00  7791.0  \n",
       "CLASS-368          229.573718    33.0   781.50   812.0   839.50  1499.0  \n",
       "CLASS-453         2990.524748   554.0   748.00  6493.0  6497.50  6503.0  \n",
       "CLASS-489            1.707825    26.0    27.50    28.5    29.25    30.0  \n",
       "CLASS-49            52.375930   622.0   758.50   760.0   761.50   763.0  \n",
       "CLASS-50            73.320772  2016.0  2083.75  2147.5  2211.25  2270.0  \n",
       "CLASS-51           187.144079  2272.0  2435.50  2600.5  2757.25  2913.0  \n",
       "CLASS-522           46.316265  1038.0  1080.00  1114.0  1159.00  1197.0  \n",
       "CLASS-559          613.849273  2916.0  3459.00  3987.0  4524.00  5052.0  \n",
       "CLASS-571            7.938208  7986.0  7994.25  8000.5  8005.75  8012.0  \n",
       "CLASS-606            6.075381  1199.0  1202.00  1210.0  1213.00  1216.0  \n",
       "CLASS-629          623.603540    41.0    83.50  1280.0  1325.50  1363.0  \n",
       "CLASS-651           78.429923  1218.0  1378.75  1405.0  1426.25  1446.0  \n",
       "CLASS-668            7.017378  1463.0  1472.00  1477.0  1482.00  1485.0  \n",
       "CLASS-671         3538.938054   483.0   841.00  7864.5  7896.75  7978.0  \n",
       "CLASS-720           38.322552  7792.0  7818.00  7847.0  7874.00  7922.0  \n",
       "CLASS-74           307.678264  5071.0  6249.50  6326.5  6341.50  6490.0  \n",
       "CLASS-75           413.303971  5054.0  5392.00  5734.0  6096.00  6486.0  \n",
       "CLASS-758         3691.611346  1511.0  4666.50  7822.0  7903.50  7985.0  \n",
       "CLASS-764          100.084240  1666.0  1758.50  1838.0  1929.50  2012.0  \n",
       "CLASS-765           11.783300  1235.0  1242.50  1254.0  1262.50  1272.0  \n",
       "CLASS-783            0.707107  6491.0  6491.25  6491.5  6491.75  6492.0  \n",
       "CLASS-784           34.860625     1.0     6.00    11.0    19.00   120.0  \n",
       "CLASS-804           94.515098   521.0   744.25   868.0   879.75   893.0  \n",
       "CLASS-816            0.707107    38.0    38.25    38.5    38.75    39.0  \n",
       "CLASS-839            5.182564  1447.0  1450.00  1454.0  1459.00  1462.0  \n",
       "CLASS-913         2269.096201    31.0  1504.00  1781.0  1927.00  7981.0  \n",
       "CLASS-942           44.876315  1513.0  1552.00  1587.0  1627.50  1665.0  \n",
       "CLASS-947            0.707107   665.0   665.25   665.5   665.75   666.0  \n",
       "CLASS-95            11.104400   587.0   594.50   605.0   614.50   621.0  \n",
       "CLASS-953            2.900123  1365.0  1366.75  1368.5  1371.25  1373.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.groupby(('Product_Category')).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5719\n"
     ]
    }
   ],
   "source": [
    "list_of_descriptions,uniques=transform_data(train_data)\n",
    "print(len(list_of_descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "l=0\n",
    "for i in list_of_descriptions:\n",
    "    if len(i)>l:\n",
    "        l=len(i)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model=generate_embeddings(list_of_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 28, 100)           166600    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 38)                3838      \n",
      "=================================================================\n",
      "Total params: 250,838\n",
      "Trainable params: 84,238\n",
      "Non-trainable params: 166,600\n",
      "_________________________________________________________________\n",
      "Train on 3495 samples, validate on 1080 samples\n",
      "Epoch 1/10\n",
      " - 8s - loss: 0.0122 - acc: 0.6913 - val_loss: 0.0048 - val_acc: 0.8694\n",
      "Epoch 2/10\n",
      " - 7s - loss: 0.0035 - acc: 0.9093 - val_loss: 0.0031 - val_acc: 0.9352\n",
      "Epoch 3/10\n",
      " - 7s - loss: 0.0026 - acc: 0.9411 - val_loss: 0.0027 - val_acc: 0.9370\n",
      "Epoch 4/10\n",
      " - 7s - loss: 0.0022 - acc: 0.9474 - val_loss: 0.0026 - val_acc: 0.9426\n",
      "Epoch 5/10\n",
      " - 7s - loss: 0.0021 - acc: 0.9505 - val_loss: 0.0021 - val_acc: 0.9481\n",
      "Epoch 6/10\n",
      " - 7s - loss: 0.0017 - acc: 0.9594 - val_loss: 0.0033 - val_acc: 0.8963\n",
      "Epoch 7/10\n",
      " - 7s - loss: 0.0016 - acc: 0.9674 - val_loss: 0.0017 - val_acc: 0.9685\n",
      "Epoch 8/10\n",
      " - 7s - loss: 0.0012 - acc: 0.9780 - val_loss: 0.0015 - val_acc: 0.9704\n",
      "Epoch 9/10\n",
      " - 7s - loss: 0.0011 - acc: 0.9785 - val_loss: 0.0020 - val_acc: 0.9593\n",
      "Epoch 10/10\n",
      " - 7s - loss: 0.0012 - acc: 0.9745 - val_loss: 0.0013 - val_acc: 0.9704\n",
      "Training Accuracy: 0.9801\n",
      "Testing Accuracy:  0.0944\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 28, 100)           166600    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 38)                3838      \n",
      "=================================================================\n",
      "Total params: 250,838\n",
      "Trainable params: 84,238\n",
      "Non-trainable params: 166,600\n",
      "_________________________________________________________________\n",
      "Train on 3506 samples, validate on 1069 samples\n",
      "Epoch 1/10\n",
      " - 8s - loss: 0.0137 - acc: 0.6175 - val_loss: 0.0066 - val_acc: 0.8195\n",
      "Epoch 2/10\n",
      " - 7s - loss: 0.0070 - acc: 0.7995 - val_loss: 0.0058 - val_acc: 0.8400\n",
      "Epoch 3/10\n",
      " - 7s - loss: 0.0054 - acc: 0.8591 - val_loss: 0.0039 - val_acc: 0.9018\n",
      "Epoch 4/10\n",
      " - 7s - loss: 0.0041 - acc: 0.8999 - val_loss: 0.0034 - val_acc: 0.9224\n",
      "Epoch 5/10\n",
      " - 7s - loss: 0.0035 - acc: 0.9199 - val_loss: 0.0029 - val_acc: 0.9364\n",
      "Epoch 6/10\n",
      " - 7s - loss: 0.0029 - acc: 0.9324 - val_loss: 0.0023 - val_acc: 0.9448\n",
      "Epoch 7/10\n",
      " - 7s - loss: 0.0026 - acc: 0.9381 - val_loss: 0.0024 - val_acc: 0.9411\n",
      "Epoch 8/10\n",
      " - 7s - loss: 0.0025 - acc: 0.9410 - val_loss: 0.0021 - val_acc: 0.9495\n",
      "Epoch 9/10\n",
      " - 7s - loss: 0.0023 - acc: 0.9501 - val_loss: 0.0018 - val_acc: 0.9560\n",
      "Epoch 10/10\n",
      " - 8s - loss: 0.0017 - acc: 0.9589 - val_loss: 0.0013 - val_acc: 0.9710\n",
      "Training Accuracy: 0.9714\n",
      "Testing Accuracy:  0.2316\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 28, 100)           166600    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 38)                3838      \n",
      "=================================================================\n",
      "Total params: 250,838\n",
      "Trainable params: 84,238\n",
      "Non-trainable params: 166,600\n",
      "_________________________________________________________________\n",
      "Train on 3531 samples, validate on 1044 samples\n",
      "Epoch 1/10\n",
      " - 11s - loss: 0.0155 - acc: 0.5477 - val_loss: 0.0089 - val_acc: 0.7404\n",
      "Epoch 2/10\n",
      " - 8s - loss: 0.0069 - acc: 0.8117 - val_loss: 0.0060 - val_acc: 0.8429\n",
      "Epoch 3/10\n",
      " - 7s - loss: 0.0050 - acc: 0.8743 - val_loss: 0.0048 - val_acc: 0.8707\n",
      "Epoch 4/10\n",
      " - 7s - loss: 0.0044 - acc: 0.8853 - val_loss: 0.0042 - val_acc: 0.8898\n",
      "Epoch 5/10\n",
      " - 7s - loss: 0.0039 - acc: 0.9128 - val_loss: 0.0037 - val_acc: 0.9253\n",
      "Epoch 6/10\n",
      " - 7s - loss: 0.0029 - acc: 0.9354 - val_loss: 0.0034 - val_acc: 0.9262\n",
      "Epoch 7/10\n",
      " - 8s - loss: 0.0026 - acc: 0.9445 - val_loss: 0.0030 - val_acc: 0.9320\n",
      "Epoch 8/10\n",
      " - 9s - loss: 0.0023 - acc: 0.9462 - val_loss: 0.0028 - val_acc: 0.9330\n",
      "Epoch 9/10\n",
      " - 16s - loss: 0.0021 - acc: 0.9499 - val_loss: 0.0024 - val_acc: 0.9349\n",
      "Epoch 10/10\n",
      " - 12s - loss: 0.0021 - acc: 0.9510 - val_loss: 0.0025 - val_acc: 0.9339\n",
      "Training Accuracy: 0.9482\n",
      "Testing Accuracy:  1.0000\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 28, 100)           166600    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 38)                3838      \n",
      "=================================================================\n",
      "Total params: 250,838\n",
      "Trainable params: 84,238\n",
      "Non-trainable params: 166,600\n",
      "_________________________________________________________________\n",
      "Train on 3536 samples, validate on 1039 samples\n",
      "Epoch 1/10\n",
      " - 12s - loss: 0.0153 - acc: 0.5461 - val_loss: 0.0073 - val_acc: 0.7729\n",
      "Epoch 2/10\n",
      " - 10s - loss: 0.0071 - acc: 0.7735 - val_loss: 0.0058 - val_acc: 0.8412\n",
      "Epoch 3/10\n",
      " - 9s - loss: 0.0055 - acc: 0.8411 - val_loss: 0.0043 - val_acc: 0.8874\n",
      "Epoch 4/10\n",
      " - 9s - loss: 0.0041 - acc: 0.8959 - val_loss: 0.0029 - val_acc: 0.9413\n",
      "Epoch 5/10\n",
      " - 7s - loss: 0.0029 - acc: 0.9321 - val_loss: 0.0028 - val_acc: 0.9374\n",
      "Epoch 6/10\n",
      " - 7s - loss: 0.0028 - acc: 0.9350 - val_loss: 0.0021 - val_acc: 0.9509\n",
      "Epoch 7/10\n",
      " - 8s - loss: 0.0020 - acc: 0.9528 - val_loss: 0.0018 - val_acc: 0.9577\n",
      "Epoch 8/10\n",
      " - 8s - loss: 0.0018 - acc: 0.9562 - val_loss: 0.0016 - val_acc: 0.9605\n",
      "Epoch 9/10\n",
      " - 9s - loss: 0.0018 - acc: 0.9545 - val_loss: 0.0013 - val_acc: 0.9634\n",
      "Epoch 10/10\n",
      " - 11s - loss: 0.0016 - acc: 0.9613 - val_loss: 0.0016 - val_acc: 0.9519\n",
      "Training Accuracy: 0.9504\n",
      "Testing Accuracy:  0.9685\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 28, 100)           166600    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 38)                3838      \n",
      "=================================================================\n",
      "Total params: 250,838\n",
      "Trainable params: 84,238\n",
      "Non-trainable params: 166,600\n",
      "_________________________________________________________________\n",
      "Train on 3551 samples, validate on 1025 samples\n",
      "Epoch 1/10\n",
      " - 14s - loss: 0.0141 - acc: 0.5936 - val_loss: 0.0079 - val_acc: 0.7805\n",
      "Epoch 2/10\n",
      " - 9s - loss: 0.0051 - acc: 0.8502 - val_loss: 0.0037 - val_acc: 0.9249\n",
      "Epoch 3/10\n",
      " - 10s - loss: 0.0031 - acc: 0.9273 - val_loss: 0.0024 - val_acc: 0.9454\n",
      "Epoch 4/10\n",
      " - 8s - loss: 0.0022 - acc: 0.9499 - val_loss: 0.0015 - val_acc: 0.9629\n",
      "Epoch 5/10\n",
      " - 8s - loss: 0.0017 - acc: 0.9600 - val_loss: 0.0013 - val_acc: 0.9707\n",
      "Epoch 6/10\n",
      " - 10s - loss: 0.0014 - acc: 0.9656 - val_loss: 0.0011 - val_acc: 0.9737\n",
      "Epoch 7/10\n",
      " - 7s - loss: 0.0013 - acc: 0.9696 - val_loss: 9.7628e-04 - val_acc: 0.9756\n",
      "Epoch 8/10\n",
      " - 7s - loss: 0.0016 - acc: 0.9631 - val_loss: 0.0010 - val_acc: 0.9756\n",
      "Epoch 9/10\n",
      " - 8s - loss: 0.0012 - acc: 0.9707 - val_loss: 0.0010 - val_acc: 0.9746\n",
      "Epoch 10/10\n",
      " - 7s - loss: 0.0010 - acc: 0.9755 - val_loss: 0.0012 - val_acc: 0.9727\n",
      "Training Accuracy: 0.9694\n",
      "Testing Accuracy:  0.0884\n"
     ]
    }
   ],
   "source": [
    "k=KFold(n_splits=5)\n",
    "ml_models_cross_valid=dict()\n",
    "for x,y in k.split(train_data['Item_Description']):\n",
    "    train_X,test_X=train_data['Item_Description'].values[x],train_data['Item_Description'].values[y]\n",
    "    train_y,test_y=train_data['Product_Category'].values[x],train_data['Product_Category'].values[y]\n",
    "    train_X,test_X,tokenizer=tokenize_input_data(train_X,test_X)\n",
    "    train_X,test_X=pad_sentences(train_X,test_X,28)\n",
    "    embedding_matrix=create_embedding_matrix(embedding_model,tokenizer)\n",
    "    ml_model=define_learning_model(embedding_model,embedding_matrix,28)\n",
    "    train_y,test_y=label_to_id(train_y,test_y)\n",
    "    history,accuracy=calculate_accuracy(train_X,train_y,test_X,test_y,ml_model)\n",
    "    ml_models_cross_valid[ml_model]=accuracy\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=pd.read_csv('Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['Item_Description']=test_data['Item_Description'].replace(np.nan,'',regex=True)\n",
    "test_data['Item_Description'] = test_data['Item_Description'].apply(preprocessor)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_x,t_y,tok_t=tokenize_input_data(test_data['Item_Description'],test_data['Item_Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2292, 28)\n"
     ]
    }
   ],
   "source": [
    "t_x,t_y=pad_sentences(t_x,t_y,28)\n",
    "print(t_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_acc=0\n",
    "for x,y in ml_models_cross_valid.items():\n",
    "    if y>max_acc:\n",
    "        max_acc=y\n",
    "        ml_model=x\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=ml_model.predict_proba(t_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_op=list()\n",
    "for x in y_pred:\n",
    "    final_op.append(uniques[np.argmax(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['Product_Category']=pd.Series(final_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Inv_Id  Vendor_Code     GL_Code    Inv_Amt  \\\n",
      "0       6  VENDOR-1197  GL-6050100  10.916343   \n",
      "1      12   VENDOR-792  GL-6050100  38.658772   \n",
      "2      14   VENDOR-792  GL-6050100  46.780476   \n",
      "3      18   VENDOR-792  GL-6050100   7.058866   \n",
      "4      19   VENDOR-792  GL-6050100  32.931765   \n",
      "\n",
      "                                    Item_Description Product_Category  \n",
      "0  [desoto, inc, store, management, real, estate,...        CLASS-323  \n",
      "1  [century, realty, trust, store, management, re...        CLASS-323  \n",
      "2  [century, realty, trust, store, management, re...        CLASS-323  \n",
      "3  [century, realty, trust, store, management, re...        CLASS-323  \n",
      "4  [century, realty, trust, store, management, re...        CLASS-323  \n"
     ]
    }
   ],
   "source": [
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[['Inv_Id','Product_Category']].to_csv('output.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

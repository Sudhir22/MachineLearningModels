{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sudhir/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#pip install tensorflow-gpu\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import gensim\n",
    "import tensorflow as tf\n",
    "from keras import Sequential\n",
    "from keras import layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import utils\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend\n",
    "\n",
    "#backend.tensorflow_backend._get_available_gpus()\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Loading the data into a dataframe\n",
    "    \n",
    "    Input\n",
    "    path: path to the test data(String)\n",
    "    \n",
    "    Output\n",
    "    train_data: return a pandas Dataframe\n",
    "    \"\"\"\n",
    "    train_data=pd.read_csv(path)\n",
    "    print(train_data.head())\n",
    "    return train_data\n",
    "\n",
    "#referenced from https://stackoverflow.com/questions/16645799/how-to-create-a-word-cloud-from-a-corpus-in-python\n",
    "def show_wordcloud(data, title = None):\n",
    "    \"\"\"\n",
    "    depicting wordclouds of the input data\n",
    "    \n",
    "    Input\n",
    "    data: input pandas Dataframe\n",
    "    \"\"\"\n",
    "    stopwords = set(STOPWORDS)\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='white',\n",
    "        stopwords=stopwords,\n",
    "        max_words=200,\n",
    "        max_font_size=40, \n",
    "        scale=3,\n",
    "        random_state=1 # chosen at random by flipping a coin; it was heads\n",
    "    ).generate(str(data))\n",
    "\n",
    "    fig = plt.figure(1, figsize=(12, 12))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "\n",
    "def preprocessor(text):\n",
    "    \"\"\"\n",
    "    Tokenizing the sentences using regular expressions and NLTK library\n",
    "    \n",
    "    Input\n",
    "    text: list of descriptions\n",
    "    \n",
    "    Output:\n",
    "    alphabet_tokens: list of tokens\n",
    "    \"\"\"\n",
    "    __tokenization_pattern = r'''(?x)          # set flag to allow verbose regexps\n",
    "        \\$?\\d+(?:\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
    "      | (?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "      | \\w+(?:-\\w+)*        # words with optional internal hyphens\n",
    "      | \\.\\.\\.              # ellipsis\n",
    "      | [][.,;\"'?():_`-]    # these are separate tokens; includes ], [\n",
    "    '''\n",
    "\n",
    "    ## call it using tokenizer.tokenize\n",
    "    tokenizer = nltk.tokenize.regexp.RegexpTokenizer(__tokenization_pattern)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens=[token.lower() for token in tokens if token.isalpha()]\n",
    "    alphabet_tokens = [token for token in tokens if token.isalpha()]\n",
    "    #en_stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    #non_stopwords = [word for word in alphabet_tokens if not word in en_stopwords]\n",
    "    #stemmer = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "    #stems = [str(stemmer.stem(word)) for word in non_stopwords]\n",
    "\n",
    "    return list(alphabet_tokens)\n",
    "\n",
    "\n",
    "\n",
    "def transform_data(data):\n",
    "    \"\"\"\n",
    "    Factorizing the simplified lithologies into numerical equivalents\n",
    "    \n",
    "    Input\n",
    "    data: input pandas dataframe\n",
    "    \n",
    "    Output\n",
    "    tuple containing the transformed data\n",
    "    \"\"\"\n",
    "    train_data['EssayText']=train_data['EssayText'].replace(np.nan,'',regex=True)\n",
    "    train_data['EssayText'] = train_data['EssayText'].apply(preprocessor)\n",
    "    list_of_descriptions=train_data['EssayText'].tolist()\n",
    "    return list_of_descriptions\n",
    "\n",
    "\n",
    "def generate_embeddings(list_of_descriptions):\n",
    "    \"\"\"\n",
    "    Generating word2vec(vectorized version of each word) model from the vocabulary in the data\n",
    "    \n",
    "    Input\n",
    "    list_of_descriptions: transformed descriptions\n",
    "    list_of_simple_lithology: transformed simple lithologies\n",
    "    \n",
    "    Output\n",
    "    model: Gensim word2vec model\n",
    "\n",
    "    \"\"\"\n",
    "    data=[]\n",
    "    for x in list_of_descriptions:\n",
    "        temp=[]\n",
    "        if(isinstance(x,list)):\n",
    "            for y in x:\n",
    "                temp.append(y.lower())\n",
    "            data.append(temp)\n",
    "    model=gensim.models.FastText(data,min_count=1,size=100,window=3)\n",
    "    return model\n",
    "\n",
    "def split_data(train_data):\n",
    "    \"\"\"\n",
    "    Splitting the data into train and test\n",
    "    \n",
    "    Input\n",
    "    train_data: Pandas dataframe\n",
    "    \n",
    "    Output\n",
    "    tuple containing train and test data \n",
    "    \"\"\"\n",
    "    msk = np.random.rand(len(train_data)) < 0.75\n",
    "    train_X = train_data.EssayText[msk]\n",
    "    test_X = train_data.EssayText[~msk]\n",
    "    y=train_data['avg_score']\n",
    "    train_y = y[msk]\n",
    "    test_y = y[~msk]\n",
    "\n",
    "    return (train_X,train_y,test_X,test_y)\n",
    "\n",
    "\n",
    "def tokenize_input_data(train_X,test_X):\n",
    "    \"\"\"\n",
    "    Indexing each token in the descriptions\n",
    "    \n",
    "    Input\n",
    "    train_X: list of input descriptions\n",
    "    test_X : list of input descriptions\n",
    "    \n",
    "    Output\n",
    "    Tuple containing indexed versions of the inputs\n",
    "    \"\"\"\n",
    "    tokenizer_x=Tokenizer(num_words=3000)    \n",
    "    tokenizer_x.fit_on_texts(train_X)\n",
    "    train_X_transformed=tokenizer_x.texts_to_sequences(train_X)\n",
    "    test_X_transformed=tokenizer_x.texts_to_sequences(test_X)\n",
    "    return (train_X_transformed,test_X_transformed,tokenizer_x)\n",
    "\n",
    "def label_to_id(train_y,test_y):\n",
    "    \"\"\"\n",
    "    Indexing each label in the target(simplified lithology)\n",
    "    \n",
    "    Input\n",
    "    train_y: list of labels\n",
    "    test_y: list of labels\n",
    "    \n",
    "    Output\n",
    "    tuple containing indexed versions of the input\n",
    "    \"\"\"\n",
    "    train_y_transformed=utils.to_categorical(train_y.tolist(),14,dtype='int')\n",
    "    test_y_transformed=utils.to_categorical(test_y.tolist(),14,dtype='int')\n",
    "    return (train_y_transformed,test_y_transformed)\n",
    "\n",
    "\n",
    "def pad_sentences(train_X,test_X,maxlen):\n",
    "    \"\"\"\n",
    "    Adding padding to the descriptions so that each description is of the same length(maxlen)\n",
    "    \n",
    "    Input\n",
    "    train_X: list of descriptions\n",
    "    test_X: list of descriptions\n",
    "    maxlen: int (maximum length of the descriptions)\n",
    "    \n",
    "    Output\n",
    "    Tuple containing transformed versions of the input\n",
    "    \"\"\"\n",
    "    train_X_transformed= pad_sequences(train_X, padding='post', maxlen=maxlen)\n",
    "    test_X_transformed= pad_sequences(test_X, padding='post', maxlen=maxlen)\n",
    "    return (train_X_transformed,test_X_transformed)\n",
    "    \n",
    "\n",
    "def create_embedding_matrix(model,tokenizer):\n",
    "    \"\"\"\n",
    "    Creating an embedding matrix to be fed into the neural network\n",
    "    \n",
    "    Input\n",
    "    model: gensim word2vec model\n",
    "    \n",
    "    embedding_matrix: matrix depicting the embeddings\n",
    "    \"\"\"\n",
    "    embedding_matrix=np.zeros((len(model.wv.vocab),100))\n",
    "    for x,y in model.wv.vocab.items():\n",
    "        if x in tokenizer.word_counts.keys():\n",
    "            embedding_matrix[tokenizer.word_index[x]]=np.array(model.wv[x], dtype=np.float32)[:100]\n",
    "    \n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def define_learning_model(model,embedding_matrix,maxlen):\n",
    "    \"\"\"\n",
    "    Describing the deep learning model using Keras\n",
    "    \n",
    "    Input\n",
    "    model:gensim word2vec model\n",
    "    embedding_matrix: matrix of embeddings\n",
    "    maxlen: maximum length of sentences\n",
    "    \n",
    "    Output\n",
    "    lstm_model: deep learning model\n",
    "    \"\"\"\n",
    "    lstm_model=Sequential()\n",
    "    lstm_model.add(layers.Embedding(len(model.wv.vocab), 100, \n",
    "                               weights=[embedding_matrix],\n",
    "                               input_length=maxlen,\n",
    "                               trainable=False))\n",
    "    lstm_model.add(layers.LSTM(100))\n",
    "    #model.add(layers.Dropout(0.3))\n",
    "    #model.add(layers.LSTM(100,activation='tanh',recurrent_activation='sigmoid'))\n",
    "    lstm_model.add(layers.Dropout(0.3))\n",
    "\n",
    "    #model.add(layers.GlobalAveragePooling1D())\n",
    "    lstm_model.add(layers.Dense(1,activation='linear'))\n",
    "    #model.add(layers.Flatten())\n",
    "    adam=optimizers.Adam(lr=0.001)\n",
    "    lstm_model.compile(optimizer=adam,\n",
    "                  loss='mse',\n",
    "                  metrics=['accuracy'])\n",
    "    lstm_model.summary()\n",
    "    return lstm_model\n",
    "\n",
    "def calculate_accuracy(train_X,train_y,model):\n",
    "    \"\"\"\n",
    "    Calculating the accuracy of the model.\n",
    "    \n",
    "    Input\n",
    "    train_X: list of descriptions\n",
    "    train_y: list of labels\n",
    "    \n",
    "    Output:\n",
    "    history: model after fitting the data\n",
    "    \n",
    "    \"\"\"\n",
    "    msk=np.random.randn(len(train_X))<0.75\n",
    "    validation_data_X=train_X[~msk]\n",
    "    validation_data_Y=train_y[~msk]\n",
    "    history = model.fit(train_X[msk],train_y[msk],\n",
    "                        epochs=10,\n",
    "                        verbose=2,\n",
    "                       validation_data=(validation_data_X,validation_data_Y))\n",
    "    loss, accuracy = model.evaluate(train_X, train_y, verbose=False)\n",
    "    print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "    loss, accuracy = model.evaluate(test_X, test_y, verbose=False)\n",
    "    print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "    \n",
    "    return history\n",
    "\n",
    "#used as reference from https://www.tensorflow.org/tutorials/keras/basic_text_classification\n",
    "def plot_loss(model):\n",
    "    \"\"\"\n",
    "    Plot the training and validation loss w.r.t epochs\n",
    "    \n",
    "    Input\n",
    "    model: deep learning model\n",
    "    \"\"\"\n",
    "    history_dict = history.history\n",
    "    history_dict.keys()\n",
    "    loss = history_dict['loss']\n",
    "    val_loss = history_dict['val_loss']\n",
    "\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "\n",
    "    # \"bo\" is for \"blue dot\"\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    # b is for \"solid blue line\"\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID  Essayset  min_score  max_score  score_1  score_2  score_3  score_4  \\\n",
      "0   1       1.0          0          3        1        1      1.0      1.0   \n",
      "1   2       1.0          0          3        1        1      NaN      1.5   \n",
      "2   3       1.0          0          3        1        1      1.0      1.0   \n",
      "3   4       1.0          0          3        0        0      0.0      0.0   \n",
      "4   5       1.0          0          3        2        2      2.0      2.5   \n",
      "\n",
      "   score_5        clarity       coherent  \\\n",
      "0      1.0        average          worst   \n",
      "1      1.0      excellent          worst   \n",
      "2      1.5          worst  above_average   \n",
      "3      1.0          worst          worst   \n",
      "4      1.0  above_average          worst   \n",
      "\n",
      "                                           EssayText  \n",
      "0  Some additional information that we would need...  \n",
      "1  After reading the expirement, I realized that ...  \n",
      "2  What you need is more trials, a control set up...  \n",
      "3  The student should list what rock is better an...  \n",
      "4  For the students to be able to make a replicat...  \n"
     ]
    }
   ],
   "source": [
    "train_data=load_data('../Q2/train_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['avg_score']=train_data[['score_1','score_2','score_3','score_4','score_5']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['clarity']=pd.Categorical(train_data['clarity'])\n",
    "train_data['clarity']=train_data['clarity'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['coherent']=pd.Categorical(train_data['coherent'])\n",
    "train_data['coherent']=train_data['coherent'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM for using features apart from the essay to calculate the final score\n",
    "msk = np.random.rand(len(train_data)) < 0.75\n",
    "train_X_svm = train_data[['clarity','coherent']][msk]\n",
    "test_X_svm = train_data[['clarity','coherent']][~msk]\n",
    "y=train_data['avg_score']\n",
    "train_y_svm = y[msk]\n",
    "test_y_svm = y[~msk]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=svm.SVR(C=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=10, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
       "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train_X_svm,train_y_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19544847382324498\n"
     ]
    }
   ],
   "source": [
    "y_pred=clf.predict(test_X_svm)\n",
    "print(mean_squared_error(test_y_svm,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_essays=transform_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model=generate_embeddings(list_of_essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X,train_y,test_X,test_y=split_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X,test_X,tokenizer=tokenize_input_data(train_X,test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X,test_X=pad_sentences(train_X,test_X,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix=create_embedding_matrix(embedding_model,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 100)          1536600   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,617,101\n",
      "Trainable params: 80,501\n",
      "Non-trainable params: 1,536,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ml_model=define_learning_model(embedding_model,embedding_matrix,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9926 samples, validate on 2864 samples\n",
      "Epoch 1/10\n",
      " - 69s - loss: 0.5149 - acc: 0.1573 - val_loss: 0.4197 - val_acc: 0.1432\n",
      "Epoch 2/10\n",
      " - 83s - loss: 0.3884 - acc: 0.1905 - val_loss: 0.3423 - val_acc: 0.1878\n",
      "Epoch 3/10\n",
      " - 62s - loss: 0.3557 - acc: 0.1968 - val_loss: 0.3421 - val_acc: 0.1920\n",
      "Epoch 4/10\n",
      " - 66s - loss: 0.3337 - acc: 0.2011 - val_loss: 0.3114 - val_acc: 0.1997\n",
      "Epoch 5/10\n",
      " - 64s - loss: 0.3175 - acc: 0.2089 - val_loss: 0.3233 - val_acc: 0.1858\n",
      "Epoch 6/10\n",
      " - 65s - loss: 0.3057 - acc: 0.2104 - val_loss: 0.3282 - val_acc: 0.2025\n",
      "Epoch 7/10\n",
      " - 63s - loss: 0.2925 - acc: 0.2169 - val_loss: 0.3361 - val_acc: 0.1910\n",
      "Epoch 8/10\n",
      " - 66s - loss: 0.2841 - acc: 0.2206 - val_loss: 0.2807 - val_acc: 0.2109\n",
      "Epoch 9/10\n",
      " - 62s - loss: 0.2670 - acc: 0.2257 - val_loss: 0.2918 - val_acc: 0.1994\n",
      "Epoch 10/10\n",
      " - 63s - loss: 0.2599 - acc: 0.2273 - val_loss: 0.2703 - val_acc: 0.2088\n",
      "Training Accuracy: 0.2292\n",
      "Testing Accuracy:  0.2062\n"
     ]
    }
   ],
   "source": [
    "history=calculate_accuracy(train_X,train_y,ml_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ID  Essayset  min_score  max_score        clarity       coherent  \\\n",
      "0  1673         1          0          3        average          worst   \n",
      "1  1674         1          0          3        average          worst   \n",
      "2  1675         1          0          3  above_average  above_average   \n",
      "3  1676         1          0          3          worst          worst   \n",
      "4  1677         1          0          3          worst          worst   \n",
      "\n",
      "                                           EssayText  \n",
      "0  The procedures I think they should have includ...  \n",
      "1  In order to replicate this experiment, you wou...  \n",
      "2  In order to replicate their experiment, you wo...  \n",
      "3  Pleace a simple of one material into one conta...  \n",
      "4  Determin the mass of four different samples ma...  \n"
     ]
    }
   ],
   "source": [
    "test_data=load_data('../Q2/test_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['EssayText']=test_data['EssayText'].replace(np.nan,'',regex=True)\n",
    "test_data['EssayText']=test_data['EssayText'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['clarity']=pd.Categorical(test_data['clarity'])\n",
    "test_data['clarity']=test_data['clarity'].cat.codes\n",
    "test_data['coherent']=pd.Categorical(test_data['coherent'])\n",
    "test_data['coherent']=test_data['coherent'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_pred_svm=clf.predict(test_data[['clarity','coherent']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=tokenizer.texts_to_sequences(test_data['EssayText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5224, 100)\n",
      "(12790, 100)\n"
     ]
    }
   ],
   "source": [
    "test=pad_sequences(test,padding='post',maxlen=100)\n",
    "print(test.shape)\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_pred_lstm=ml_model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score=[]\n",
    "i=0\n",
    "for (x,y),value in np.ndenumerate(score_pred_lstm):\n",
    "    final_score.append(0.3*value+0.7*score_pred_svm[i])\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['essay_score']=pd.Series(final_score)\n",
    "test_data['essay_score']=test_data['essay_score'].apply(np.round).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ID  Essayset  min_score  max_score  clarity  coherent  \\\n",
      "0  1673         1          0          3        1         3   \n",
      "1  1674         1          0          3        1         3   \n",
      "2  1675         1          0          3        0         0   \n",
      "3  1676         1          0          3        3         3   \n",
      "4  1677         1          0          3        3         3   \n",
      "\n",
      "                                           EssayText  essay_score  \n",
      "0  [the, procedures, i, think, they, should, have...            0  \n",
      "1  [in, order, to, replicate, this, experiment, y...            1  \n",
      "2  [in, order, to, replicate, their, experiment, ...            2  \n",
      "3  [pleace, a, simple, of, one, material, into, o...            0  \n",
      "4  [determin, the, mass, of, four, different, sam...            0  \n"
     ]
    }
   ],
   "source": [
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[['ID','Essayset','essay_score']].to_csv('output.csv',index=False,header=['id','essay_set','essay_score'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
